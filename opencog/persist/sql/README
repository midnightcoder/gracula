                                Persist
                                -------
                   Linas Vepstas <linasvepstas@gmail.com>
                    Basic implementation Feb-June 2008
                         Status update May 2009

A simple-minded implementation of atom persistence into SQL.

STATUS:
Functional but incomplete prototype. Save and restore of atoms, and 
several kinds of truth values, works. Bulk save-and-restore of entire
AtomTable contents works.  Incremental save/restore (i.e.  update the
SQL contents as AtomTable changes) works, but is not fully automatic 
(needs resolution of deeper AtomSpace issues.)

Missing features/ToDo items: 
Add full support for CompositeTruthValue.
Add full support for attention values. (??)
Add full support for Atom deletion.
Add full support for 64-bit systems, including >4Gigs of atoms.
Provide optimized table layout for EvaluationLinks.

Performance status:
AtomTable load time is more than 3x faster than loading from NMXml;
Also, loading from SQL reduces cogServer memory usage by 3x.
While this suggests that the NMXml loader fragments RAM in a bad way,
it now seems that the true culprit is CSockets layer, which appears
to leak memory under certain scenarios.


Goals:
------
The goal of this implementation is to:

1) Simplify Linas' day-to-day use of opencog. The problem is that
   loading data, by parsing English sentences, into opencog is slow.
   Every time opencog source is changed, the data needs to be reloaded.
   This makes development tedious.  Data persistence alleviates the 
   tedium of data load time.

2) Make sure that the opencog core design is amenable to incremental,
   just-in-time data persistence; that is, the fetching of data 
   as it is needed, and saving it away when its not needed. This
   requires an infrastructure for attention allocation (atoms
   that don't get attention can be saved away to disk, while those
   that are needed are fetched on demand.) This will also require 
   infrastructure for locks, use counts, etc. that are typical
   of multi-threaded programming.  This prototype should lay the 
   foundations for more sophisticated schemes (non-SQL-based) 
   to take its place.  This architectural work is inherently difficult,
   and is ongoing.

3) Provide a baseline/reference implementation by which other 
   persistence designs can be measured. It is hoped that other systems
   would be at least as fast and as scalable as this one is: this is
   meant to provide a minimal function and performance level. The
   strength of the current design is supposed to be simplicity, not
   scalability or raw performance.


Design:
-------
The core design defines only a few very simple SQL tables, and some
simple readers and writers to save and restore atoms from an SQL 
database.

Note that the core design does *not* make use of object reflection, 
nor can it store arbitrary kinds of objects. It is very definitely 
hard-wired. Yes, this can be considered to be a short-coming. 
A more general, persistent object framework (for C) can be found 
at http://estron.alioth.debian.org/  However, simplicity, at the
cost of missing flexibility, seems more important.

The current design can save/restore individual atoms, and it can
bulk-save/bulk-restore the entire contents of an AtomTable.
A semi-realized goal of the prototype is to implement incremental save 
and restore -- that is, to fetch atoms in a "just in time" fashion, and
to save away atoms that are not needed in RAM (e.g. atoms with 
low/non-existent attention values). The AtomSpace BackingStore provides
the current, minimalistic, low-function API for this.

There are architectural issues that remain to be resolved before fully
automatic incremental save/restore can be implemented.  For example,
the current AtomSpace design assumes that the incoming set of an atom is
fully populated; this makes it hard to leave some atoms on disk while
others are in RAM.  These issues need to be resolved for *any* persistent
back-end, and are not specific to this one.

See also: the UUID collision issue, discussed below.

Caveats:
--------
The wisdom of an SQL-based design is not at all clear; there are several
major drawbacks to using SQL. These include:
-- Large performance overhead, even if an embedded SQL server were to
   be used. Other systems might have a lower overhead.
-- Efficiency of incremental save/restore is unclear; there's potentially
   large churn, if short-lived atoms are frequently created/destroyed.


Compiling
---------
There are two popular packages that provide ODBC for Linux: IODBC and 
UnixODBC. The second is preferred, because it seems that IODBC has
trouble with UTF-8 strings. It is almost impossible to avoid UTF-8 
strings in NLP input data.


Device Driver Setup
-------------------
A number of steps are required to install and configure the database
drivers used by the persistence system.

First, ODBC and drivers must be installed. The UnixODBC system is 
recommended; although iODBC "works", it does not have the UTF-8 support
needed for text-processing.  For Debian/Ubuntu, install the packages 
ODBC-postgresql and UnixODBC (Assuming you want to use Postgres). After
installing these two, edit /etc/odbcinst.ini and put the stanza below
into it.

[PostgreSQL]
Description = PostgreSQL ODBC driver for Linux and Windows (Unicode version)
Driver      = /usr/lib/odbc/psqlodbcw.so
Setup       = /usr/lib/odbc/libodbcpsqlS.so
Debug       = 0
CommLog     = 0

The above stanza associates the name 'PostgreSQL' with a particular
driver; this name is needed below.  MySQL users need the following stanza: 
a file can safely contain may stanzas defining many different drivers.

[MySQL]
Description = MySQL driver
Driver      = /usr/lib/odbc/libmyodbc.so
Setup       = /usr/lib/odbc/libodbcmyS.so
CPTimeout   =
CPReuse     =

Next, the Postgres default configuration needs to be tweaked for 
performance. Failure to do this will result in disastrous load and
store times.

Edit postgresql.conf (a typical location is 
/etc/postgresql/8.4/main/postgresql.conf) and make the following changes:

   shared_buffers = default was 32MB, change to 25% of install RAM
   effective_cache_size = default was 128MB, change to 50% of installed RAM
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100
(save file contents, then)
   sysctl -p /etc/sysctl.conf


Database Setup
--------------
This section shows how to set up the tables that will hold the data,
and the individual ODBC connection to them.

Create a database called "mycogdata"; for example, in Postgres, at
the Unix command line:

   $ createdb mycogdata

(you may need to su - postgres; createuser <username> first)
(You may need to edit pg_hba.conf)

Then create the database tables:

   $ cat atom.sql | psql mycogdata

Next, edit  ~/.odbc.ini in your home directory to add a stanza of 
the form below. You MUST create one of these for EACH repository 
you plan to use! The name of the stanza, and of the database,
can be whatever you wish.

[mycogdata]
Description       = My Favorite Database
Driver            = PostgreSQL
Trace             = No
TraceFile         =
Database          = mycogdata
Servername        = localhost
Port              = 5432
Username          = linas
Password          = asdf
ReadOnly          = No
RowVersioning     = No
ShowSystemTables  = Yes
ShowOidColumn     = Yes
FakeOidIndex      = Yes
ConnSettings      =

Be sure to adjust their username and passwd appropriately.  The password
to be supplied is the database password, not the system login password.
The database password can be set by doing:

   $ psql -c "ALTER USER linas WITH PASSWORD 'asdf'" -d template1

(Substitute your name and some appropriate password; it does not need to
be your login password, and probably shouldn't be.)


How To Pass the Unit Test
-------------------------
The collection of OpenCog unit tests includes one that tests the 
persistence infrastructure. It is called BasicSaveUTest, and it
will fail unless a test database is set up.  Toe set this up, 
you'll need to repeat some of the above instructions:

-- Create a new database: e.g. 
       $ createdb opencog-test
-- Create the tables:
       $ cat atom.sql | psql opencog-test
-- Create an entry in ~/.odbc.ini, as explained above.
-- Edit lib/opencog-test.conf, and change the database name,
   username, and password as appropriate.

After the above steps, the BasicSaveUTest should pass.


Bulk save and restore
---------------------
At last! bulk save of atoms that were previous created is done by
getting to the opencog prompt (telnet localhost 17001) and issuing the
commands:

    opencog> ?
    unknown command >>?<<
            Available commands: data dlopen dlclose exit help load ls scm
            shutdown sql-open sql-close sql-store sql-load
    opencog> sql-open
    sql-open: invalid command syntax
    Usage: sql-open <dbname> <username> <auth>

    opencog> sql-open nlp linas asdf
    Opened "nlp" as user "linas"

    opencog> sql-store
    SQL data store thread started

at this point, a progress indicator will be printed by the opencog 
server, on the OpenCog server's stdout. It  will say something like:
Stored 236000 atoms. When finished, its nice to say:

    opencog> sql-close

Next time, you can load the data with:

    opencog> sql-open nlp linas asdf
    opencog> sql-load
    SQL loader thread started

The completion message will be on the server output, for example:

    Finished loading 973300 atoms in total 


Individual-atom save and restore
--------------------------------
There is a (prototype) interface to save and restore individual atoms. 
(It is a prototype, since the details are subject to change). It may be
used as follows:

Start the server:
    $ opencog/server/cogserver -c ../lib/opencog.conf

Open a shell:
    $ telnet localhost 17001
    Trying 127.0.0.1...
    opencog>  sql-open triples linas asdf
    database opened
    opencog> scm
    Entering scheme shell; use ^D or a single . on a line by itself to exit.
    guile> (define x (ConceptNode "asdfasdf" (stv 0.123 0.789)))
    guile> (store-atom x)
    #t

The above will have caused a single atom to be stored in the database.
It's presence can be verified by examining the database directly:

    $ psql triples
    Welcome to psql 8.3.6, the PostgreSQL interactive terminal.
    triples=# select * from atoms;
     uuid | type | tv_type | stv_mean | stv_confidence | stv_count | height |   name   | outgoing 
    ------+------+---------+----------+----------------+-----------+--------+----------+----------
        2 |    3 |       1 |    0.123 |     0.78899997 | 2991.4688 |      0 | asdfasdf | 
    (1 row)

The backing-store mechanism can now automatically retrieve this atom at
a later time.  Thus, for example, shut down the server, restart it, 
re-open the database, and enter the scheme shell again. Then,

    guile> (define y (ConceptNode "asdfasdf"))
    guile> y
    (ConceptNode "asdfasdf" (stv 0.123 0.78899997))

Note that, this time, when the node was created, the database was
searched for a ConceptNode with the same string name. If it was found
in the database, then it is automatically loaded into the AtomSpace,
with the appropriate truth value taken from the database.

If the truth value is modified, the atom is *not* automatically saved
to the database (not at this time; this may change someday).  The
modified atom would need to be explicitly saved again.

Once stored, the atom may be deleted from the AtomSpace; it will
remain in storage, and can be recreated at will:

    guile> (cog-delete y)
    #t
    guile> y
    #<Invalid handle>
    guile> (define y (ConceptNode "asdfasdf"))
    guile> y
    (ConceptNode "asdfasdf" (stv 0.123 0.78899997))

The UUID associated with the atom will not change between deletions
and server restarts. This can be verified with the cog-handle command,
which returns the UUID:

    guile> (cog-handle y)
    2


Copying Databases
-----------------
Sooner or later you will want to make a copy of your database, for 
backup purposes, or sharing. Here's the current 'best practices' for 
that:

   $ pg_dump triples

That's all!

Experimental Diary & Results
----------------------------

Store performance
-----------------
This section reviews the performance for storage of data from opencog
to the SQL server (and thence to disk).

First run with a large data set (save of 1564K atoms to the database)
was a disaster.  Huge CPU usage, with 75% of CPU usage occurring in the
kernel block i/o layer, and 12% each for the opencog and postgres times:
   112:00 [md4_raid1] or 4.3 millisecs per atom
   17 minutes for postgres, and opencog, each. or 0.66 millisecs per atom
   1937576 - 1088032 kB = 850MBytes disk use

Experiment: is this due to the bad design for trying to figure whether
"INSERT" or "UPDATE" should be used? A local client-side cache of the 
keys in the DB seems to change little:

   CPU usage for postgres 11:04  and opencog 10:40 and 112:30 for md

So this change drops postgres server and opencog CPU usage
significantly, but the insane kernel CPU usage remains.

The above disaster can be attributed to bad defaults for the postgres
server. In particular, sync to disk, while critical for commercial
database use, is pointless for current use. Also, buffer sizes are much
too small. Edit postgresql.conf and make the following changes:

   shared_buffers = default was 24MB, change to 384MB
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100
(save file contents, then)
   sysctl -p /etc/sysctl.conf

After tuning, save of data to empty DB gives result:
   cogserver = 10:45 mins = 0.41  millisecs/atom (2.42K atoms/sec)
   postgres  =  7:32 mins = 0.29  millisecs/atom (2.65K atoms/sec)
   md        =  0:42 mins = 0.026 millisecs/atom (37K atoms/sec)

Try again, dropping the indexes on the atom and edge tables. Then,
after loading all atoms, rebuild the index. This time, get
   cogserver = 5:49 mins = 0.227 millisecs/atom (4.40K atoms/sec)
   postgres  = 4:50 mins = 0.189 millisecs/atom (5.30K atoms/sec)

Try again, this time with in-line outgoing sets. This improves 
performance even further:

   cogserver = 2:54 mm:ss = 0.113 millisecs/atom (8.83K atoms/sec)
   postgres  = 2:22 mm:ss = 0.092 millisecs/atom (10.82K atoms/sec)

Try again, compiled with -O3, storing to an empty table, with
no indexes on it (and with in-line outgoing sets):
   cogserver = 2:40 mm:ss
   postgres  = 2:16 mm:ss

Try again, compiled with -O3, storing to empty table, while holding 
the index on tables (and with in-line outgoing sets).

   cogserver = 2:51 mm:ss
   postgres  = 2:06 mm:ss

Apparently, the problem with the indexes has to do with holding them
for the edge table; when there's no edge table, then there's no index 
issue!?

Try again, compiled with -O3, saving to (updating) a *full* database.
(i.e. database already has the data, so we are doing UPDATE not INSERT)
   cogserver = 2:19 mm:ss
   postgres  = 4:35 mm:ss

Try again, using UnixODBC instead of iODBC, to empty table, withOUT
index tables (and -O3 and in-lined outgoing):
   cogserver = 2:36 mm:ss
   postgres  = 2:13 mm:ss

It appears that UnixODBC is essentially identical to iODBC performance

begin; commit;  
use analyze;
use prepare;

XML loading performance
-----------------------
Loading the dataset from XML files takes:

   cogserver 2:34 mm:ss when compiled without optimization
   cogserver 1:19 mm:ss when compiled with -O3 optimization

Loading performance
-------------------
Loading performance. Database contains 1564K Atoms, and 2413K edges.
CPU usage:
2:08 postgres = 82 microsecs/atom (12.2K atoms/sec)
similar to for opencog, but then AtomTable needs to reconcile, which
takes an additional 8:30 minutes to attach incoming handles!!

Conclude: database loading would be much faster if we loaded all of
the atoms first, then all of the lowest-height links, etc.  This assumes
that opencog is strictly hierarchically structured. (no "crazy loops")

After implementing height-structured restore, get following, loading
from a "hot" postgres instance (had not been stopped since previous
use, i.e. data should have been hot in RAM):
  cogserver = 2:36 mm:ss = 0.101 millisecs/atom (9.85K atoms/sec)
  postgres  = 1:59 mm:ss = 0.077 millisecs/atom (12.91K atoms/sec)

The dataset had 357162 Nodes, 1206544 Links at height 1

After a cold start, have

  cogserver = 2:32 mm:ss
  postgres  = 1:55 mm:ss

Appears that there is no performance degradation for cold-starts.

Note also: cogServer CPU usage is *identical* to its CPU usage when
loading XML! Hurrah! Also, see below: RAM usage is significantly 
reduced; apparently, the reading of XML results in very bad memory
fragmentation.

Implement in-line edges, instead of storing edges in an outboard table.

  cogserver = 41 seconds = 26.7 microsecs/atom (37.5K atoms/sec)
  postgres  =  7 seconds =  4.56 microsecs/atom (219K atoms/sec)

Turn on -O3 optimization during compile ... all previous figures
were without *any* optimization. Now get 

  cogserver = 24 seconds = 15.6 microsecs/atom (64.0K atoms/sec)
  postgres  = 11 seconds

Much much better!

10.78
23.15


Evidence of severe memory fragmentation
---------------------------------------
loading 1536K Atoms from XML results in cogserver using 1210 MBytes ram.
loading 1536K Atoms from SQL results in cogserver using 633 MBytes ram.
loading 1536K Atoms from SQL (with new record management) results in 
cogserver using 442 MBytes ram.

Almost one-third the ram !!

The fragmentation occurs when the XML is piped to cogserver much faster
than the cogserver can process it; the fragmentation is between data
buffered from the sockets, and the cogserver itself.  The fragmentation
does *not* occur when the data is piped in more slowly,i.e. at the same 
rate that opencog processes it. Alternately, the problem may be a memory
leak in CSockets; I think this is the true culprit, but that's not yet
clear.


UUID Collision
--------------
Suppose a database contains atoms with uuids from 1 to 1000.  Suppose a
user starts opencog, creates 250 atoms (thus using uuids from 1 to 251),
and then opens the database.  Unless the atoms that the user created
are *identical* to those with uuids 1..251 in the DB, then there will
be a uuid clash with the current design.

There are several possible solutions:

A) Do nothing. (i.e. keep things the way they are today.)

B) Compare range of uuids currently issued in atomspace to the range in
   the DB. If the ranges overlap, then print error and disallow opening
   of the DB.  For above scenario, 1..251 overlaps 1..1000 so there's
   an error and nothing is loaded.

C) Get max uuid used in atomspace, and call this the "uuid thunk".  
   When loading an atom from the DB, add this thunk to the UUID from
   the DB, and load the resulting atom into atomspace.  When storing
   atoms into the DB, subtract this thunk.  For above example, the 
   thunk is 251 so the DB atoms get mapped to the range 251 to 1251 in
   atomspace.

I'm about to argue that A is best :-)

But first: Note that, in the above, "DB" can mean "persistant store" 
(file or sql or other) OR some other remote, running instance of 
opencog with whom we are sharing atoms.

The problem with both B and C is that they fail if one wants to use two 
different DB's containing some of the same atoms.  So, for example, if
one loaded a list of 251 words from a file (the 1..251 range) and then 
opened a DB having the same 251 words with the same uuids appearing as 
a subrange of the 1..1000, then both B & C would fail.

The problem with A is that uuid clashes can occur.

My gut instinct is that B & C are too clever for their own pants, and
will only be regretted later on. By contrast, A is simple and can be
managed as long as one has some fore-thought and planning.


TODO
----
-- Store more CompositeTruthValues

-- Store attention values. (??)

-- Run valgrind to track down the CSockets leak problem.

-- Create custom table, tailored for EvaluationLink triples.
   Since majority of nodes/links in the DB will be in the form of 
   EvaluationLink triples, a significant performance gain can be gotten
   by creating a custom table, and shunting queries to that.  This would
   decrease the SQL table sizes significantly, and decrease server I/O 
   by factors of 2x-3x.  Another table, designed just for simple pairs,
   might help a lot, too.
