
                 Word Similarity
                 ---------------
             Linas Vepstas July 2008


This directory contains code pertaining to word similarity measures.
The similarity measures are in part driven by statistical corpus
analysis.

Frequency, probability, entropy
-------------------------------
Words, aka Unigrams -- have a frequency count, indicating how often the
word has been observed in a corpus. Given the frequency count "n", and
the total number of words seen "N", one has the relations:

  probability p = n/N
  entropy     H = log_2 p

It seems most reasonable to store the entropy in the "mean" field of the
SimpleTruthValue associated with the WordNode. 

Why Entropy?
------------
There is one primary reason to store entropy, instead of the probability
or count, and that is that entropy provides a more robust way of
managing the update of values, as new text is read in.  Consider: as
new text is read in, the relative counts/probabilities/entropies of the
observed words need to be updated. Storing a probability is impractical,
as, for every new word read, the probability for *all* words needs to be
updated, if the probabilities are to be summed to exactly 1.0.  This can 
be avoided by storing two simple counts: "n" for the word, and "N" for
all words observed.  Keeping a simple count works well, until 4 billion 
words have been read, at which point, 32-bit systems overflow.  But it
is also somewhat unappealing to keep a count "since the dawn of time", 
and it makes more sense to try to "forget" old counts, supplanting them
with only recently observed freqeuncies, via an exponentially damped
update. The trick now is how to update counts in a decaying manner, 
without acciddentally skewing proportions into meaningless-ness. 
The pragmatic answer seems to be to store entropy, and update entropy
with a decaying running average. This implies that the entropies will
be "denormalized", in that the sum over all 2^H will not add to one.
However, over relatively short periods of time, this seems to be OK,
and the entropy can be periodically renormalized so that the sum 2^H
does sum properly to one. Although, mathematically, a "denormalized"
entropy is equivalent to a set of probabilities that don't sum to one,
it is less "intellectually jarring" to work with denormalized entropies,
than to explain to a surprised newcomer why the probabilities don't add
to one.  Thus, storing entropies seems like the best way to represent
word frequencies.

Different kinds of statistics
-----------------------------
For many kinds of statstical calculations, it is not enough to store 
just a single frequency; and one needs different types of statistics
associated with a different node. Thus, for example, when computing
mutual information, one needs the number of times a word has been
observed on the left hand side of a pair, and on the right hand side
of a pair. This can be represented as follows:

  <EntropyLink>
    <WordNode name = "bark">
    <DistinguishedNode = "LeftMarginal">
  </EntropyLink> 

  <EntropyLink>
    <WordNode name = "bark">
    <DistinguishedNode = "RightMarginal">
  </EntropyLink> 

The DistinguishedNode serves as a kind-of column heading, serving the
same function as a column heading in SQL does. The actual entropy value
of to left marginal probability (resp. right marginal) is stored as a
SimpleTruthValue in the EntropyLink. The name EntropyLink simply serves
as a type to indicate what sort of a value the truth value is holding.

[An alternative would be to create two now link types:
LeftMarginalEntropyLink, and RightMarginalEntropyLink. The values would
then be associate with single links:

  <LeftMarginalEntropyLink>
    <WordNode name = "bark">
  </LeftMarginalEntropyLink>

and etc. This would use slightly less storage, at the inconvenience of
proliferating link types.]

Word Lemmas
-----------
Basic statistical information gathered from corpus analysis always
involves the frequency of words themselves, whereas for tasks such as
word similarity, one is interested in having the lemma of the word.
Thus, the goal is to represent the linkage between a word and its lemma.

   <LemmaLink>
      <WordNode name="barking" />
      <LemmaNode name="bark" />
   </LemmaLink>

TBD: what about part-of-speech ??

   <WordSenseLink>
      <LemmaNode name="bark" />
      <ConceptNode name="bark%1:20:00::" />
   </WordSenseLink>

Sense similarity
----------------
To indicate different types of similarity measures between word senses,
a multitude of different link types will be used. All of these link
types will inherit from the unordered SimilarityLink.

    <SimilarityLink strength=0.8 confidence=0.9 />
        <WordSenseNode name="bark_sense_23" />
        <WordSenseNode name="covering_sense_42" />
    </SimilarityLink>

The different types of links will be:

   -- path - simple edge counting
   -- hso - Hirst & St-Onge (1998)
   -- lch - Leacock & Chodorow (1998)
   -- lesk - Extended Gloss Overlaps (Pedersen & Banerjee 2003)
   -- lin - Lin (1998)
   -- jcn - Jiang & Conrath (1997)
   -- res - Resnik (1995)
   -- vector - Gloss Vector (Patwardhan 2003)
   -- wup - Wu & Palmer (1994)

The strength of these links will be stored in the "mean" value of a
SimpleTruthValue. These different measures are *not* normalized to
one-another -- they typically range over some range [a,b], and 
are *not* uniformly distributed. 

The links then are:
  PathSimilarityLink
  HsoSimilarityLink
  LchSimilarityLink
  LeskSimilarityLink
  LinSimilarityLink
  JcnSimilarityLink
  ResSimilarityLink
  VectorSimilarityLink
  WupSimilarityLink

Quality of Measures
-------------------
According to Budanitsky and Hirst[1], the "best" relatedness measure is
jcn, followed by lin and lch. Worst are hso and res.

According to Sinha and Mihalcea[2], the "best" for nouns is jcn, the
best for verbs is lch.

jcn is IC(p_1, p_2) -H(p_1) - H(p_2) and is thus "correct" from the 
information-theory point of view. By contrast, lin, while combining
mutual information and entropy, does so in an akward manner, from the 
information theory point of view.


Normalization
-------------
Per Sinha and Mihalcea[2]: For the lesk measure, we
observed that the edge weights were in a range from 0 up
to an arbitrary large number. Consequently, values greater
than 240 were set to 1, and the rest were mapped onto the
interval [0,1]. Similarly, the jcn values were found to range
from 0.04 to 0.2, and thus the normalization was done with
respect to this range. Finally, since the lch values ranged
from 0.34 to 3.33, they were normalized and mapped to the
[0,1] scale using this interval.



Working notes
-------------
Currently, Linas loads data as follows:
* After building an SQL database of word pairs, use the
  src/extract/export-word-count.pl script from the java lexical attraction
  package to dump the SQL database into opencog.

Curently loads 243K WordNode's -- many of these are numerical garbage,
many are foreign words; some fair amount is confused punctuation.  This
results in a cogserver instance of 158M, or 158M/243K=650 bytes/word.

References
----------
[1] Alexander Budanitsky and Graeme Hirst, "Semantic distance in WordNet:
    An experimental, application-oriented evaluation of five measures"
    (2001) In ''Proceedings of the NAACL Workshop on WordNet and Other 
    Lexical Resources'', Pittsburgh, 2001.

[2] Ravi Sinha and Rada Mihalcea, "Unsupervised Graph-based Word Sense
    Disambiguation Using Measures of Word Semantic Similarity", (2005)

