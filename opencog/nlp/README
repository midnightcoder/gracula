
                    Natural Language Processing
                    ---------------------------
                     Linas Vepstas April 2008

This directory contains a miscellany of natural language processing
tools (for the English language). Current focus is on establishing
statistical correlations between word senses and grammatical constructions,
on reference resolution, and on reasoning with prepositional relations,
and most importantly, on finding semantically important patterns.

How-To
======
The current code "doesn't do anything yet"; rather, it is a platform for
running experiments. Thus, what is contained here should be thought of
as a "bag of parts".  It is up to you to figure out what these parts
are, and to assemble them into something meaningful.  Some fragile 
examples have been constructed (e.g. the chatbot) but its up to you to
do better.

Currently, there are two basic experimental flows that are possible:
the chatbot process and the word-sense-disambiguation (WSD) process.

The chatbot process is the more ambitious of the two, combining many
different components, including "semantic triples" (nlp/triples/README)
"question answering" (nlp/question/README) and reference resolution/
concept-formation (nlp/seme/README).  The high-level overview of the 
chatbot process is given in nlp/chatbot/README.

The WSD process is the more complex, and requires creating and maintaining
large, complex databases.  The WSD process is summarized here:

 * Download and compile opencog, RelEx, lexical-attraction from
   launchpad.
 * Create a bunch of SQL tables, as specified in the lexical-attr 
   package. Configure locations and passwords in opencog.conf.
 * Parse a bunch of English text, using RelEx, to obtain frequency
   counts. Alternately, download pre-parsed texts from 
   http://gnucash.org/linas/nlp/ which contains several cpu-years
   worth of parsed data. 
 * Modify A.scm to change the hard-coded paths to point at the actual
   location of the parsed data. Modify lib/opencog-wsd.conf to 
   include A.scm on startup.  Start the cogserver.
 * Run the scripts in the wordnet-import directory to load opencog
   with basic wordnet relationships. Can save soe time by loading,
   then saving to sql database. Next time, reload from the sql 
   database, its much quicker.  (sql-open nlp )
 * Get to the scheme prompt, and run "(do-wsd 20)". This will run the
   current word-sense disambiguation code one 20 files, and will populate
   the databases with statistical results.  Caution; this is extremely 
   CPU-intensive.
 * Read the README in wsd-post and triples for latest info & results.

The lexical attraction package, at https://launchpad.net/relex-statistical
is used to define SQL tables holding assorted relationships, including
tables of mutual information between word pairs, and tables of conditional
probabilities of link-grammar disjuncts. These tables are referenced 
and/or generated by some of the opencog code here. 

Database dumps of some of the statistical datasets can be downloaded from
http://gnucash.org/linas/nlp/  These represent several CPU-years of 
number-crunching, and so are a short-cut to getting results more quickly.


Subdirectories (in alphabetical order)
======================================
chatbot
-------
The chatbot directory contains some crude code to tie opencog NLP
processing to chat systems, and particularly, to IRC chat.  Provides
a high-level implementation of a simple, hard-coded question-answering 
system, resting on the foundations of "seme" and "triples".

lexical_attraction
------------------
The lexical-attraction directory contains a README file describing some
ideas for performing pattern recognition to automatically extract 
semantic content, primarily by employing mutual information.

pln
---
Misc pln experiments.  Nothing of significance.

question
--------
C++ code that implements a basic question-answering algorithm, based on
matching the NL pattern of a question to the NL pattern of an assertion.
If the atomspace contains an assertion such as "John threw the ball", 
then this code can correctly answer the question "Who threw the ball?"
This code is now obsolete, and is not built any more. HOWEVER, the README
does explain the research results, and provides a general introduction to 
question-answering, and should be kept around indefinitely.

refres
------
Some working notes on reference resolution.
The core problem of reference resolution is determining when two words
in a text refer to the same concept. Thus, for example, the same noun,
used in neighboring sentences, probably refers to the same concept.
Alternately, anaphora (he, she, it, etc.) may also be used to refer to
the same concept.

This directory is currently empty, it is meant to someday hold an 
implementation of the Hobbs reference resolution algorithm.

See also the "seme" directory, which contains a practical, working 
implementation of reference resolution. See also the "lexical-attraction"
directory, which outlines a far more general, and hopefully far more
effective approach.

scm
---
The scm directory contains miscellaneous scheme scripts of general 
utility for NLP work. This includes scripts for pulling out link-grammar 
disjuncts from parse data.

semcor
------
The semcor directory contains some trite SemCor utilities. SemCor
is a WSJ corpus marked up with word senses.  This directory is nearly
empty and is not currently used.

seme
----
Notes on concept formation and reference resolution from linguistic input.
Contains a working implementation of basic reference resolution that is
used by the chatbot.

similarity
----------
The similarity directory contains code related to similarity of word
senses.  Describes plans/code for storing wordnet-derived similarity
measures within OpenCog.

triples
-------
The triples directory contains some experimental code for extracting
prepositional triples from text. So, for example: "Lisbon is the 
capital of Portugal" would be turned into "capital_of(Portugal,
Lisbon)".  Such "prepositional relations" extend the usual ontological 
concepts of "is_a", "has_a", "part_of" and "uses" to more general
relations.

Importantly, this directory contains code to convert plain-text
IF...THEN... relations into OpenCog ImplicationLinks. This is one
of the primary achievements of the code here.  Note that the IF...THEN..
structure used is almost identical to the RelEx Frame rules, and so
should be usable for porting RelEx frames to OpenCog.

types
-----
Scripts, etc. to load NLP-specific atom types.

wordnet-import
--------------
The wordnet-import directory contains stand-alone code (i.e. not a part
of opencog) that will walk over the wordnet database, and convert it 
into OpenCog Scheme, which can then be easily loaded into OpenCog.

wsd
---
The wsd directory contains code that implements the Rada Mihalcea 
word-sense disambiguation algorithm.  The Mihalcea algorithm assigns
senses to the words in a sentence, and constructs links between these
different senses. The links are weighted by a word-sense similarity
measure. The result is a graph whose vertices are word-senses, and 
whose edges are these links.  The graph is essentially just a Markov 
chain, and is solved as such, looking for a stationary vector of 
probabilities for the word-sense vertices. The vertices with the 
highest scores are then the most likely meaning for a word.

wsd-post
--------
The wsd-post directory contains code for generating datasets which may
be used for extremely fast (but partial) word-sense disambiguation,
based on grammatical (syntactical) usage. Generating the data-sets can
take cpu-month to cpu-years; the table-lookup can be done in milli or
microseconds.


======================================================================

Input file format
-----------------
This style of input may be produced in one of two ways. It is produced
directly by RelEx, when using the opencog output format: the -o flag
appended to the relex.RelationExtractor executable. The other way is
indirect, but can be considerably more convenient: First, parse text
using the  relex.WebFormat module, to create the "compact file format".
Then, use the relex/src/perl/cff-to-opencog.pl perl script to convert 
this to the final output. 


A Side-note About Syntactic Sugar
===================================
This has been said before, but it bears repeating. Consider the node
type WordInstanceNode, for example:

    (WordInstanceNode "cabin@99d22336-6cda-4365-8555-64260ed8bd15")

This custom-defined node type should be thought of as syntactic sugar
for the more "primitive" graph:

   (InheritenceLink
        (ConceptNode "cabin@99d22336-6cda-4365-8555-64260ed8bd15")
        (ConceptNode "WordInstance")
    )

The above InheritenceLink essentially assigns a "type" to the word
instance. This type can be used in the same way that types are
ordinarily used in other programming languages. When managing
hypergraphs, it is almost always easier and faster to locate,
manipulate and delete narrowly typed atoms.

Similarly, for links, we use the syntactic sugar

    (PartOfSpeechLink
        (WordInstanceNode "cabin@99d22336-6cda-4365-8555-64260ed8bd15")
        (DefinedLinguisticConceptNode "noun")
    )

which stands for the more "primitive" construct:

    (EvaluationLink
        (PredicateNode "PartOfSpeech"
            (ListLink
                (WordInstanceNode "cabin@99d22336-6cda-4365-8555-64260ed8bd15")
                (DefinedLinguisticConceptNode "noun")
            )
        )
    )

Notationally, these forms should be considered to be "equivalent"
although there is a bunch of actual code that depends on the one or
the other, and cannot freely intermingle these cases.

Not everything in the RelEx export gets its own specially-declared
node or link type. Those that do not are explicitly declared in the
file "src/nlp/scm/type-definitions.scm"

======================================================================
References:
-----------
"To verbize one's nouns" -- the concept of "Lexical Implication Rules": 
N. Ostler, B.T.S.Atkins, "Predictable Meaning Shift: Some Linguistic
Properties of Lexical Implication Rules", (1991) Proceedings of the
First SIGLEX Workshop on Lexical Semantics and Knowledge Representation

