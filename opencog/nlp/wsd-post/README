
                             wsd-post
                             --------
                   Linas Vepstas == December 2008
              Miscellaneous scripts for post-processing 
                 word-sense disambiguation (wsd) data.

This directory contains code for generating datasets which may be used
for extremely fast (but partial) word-sense disambiguation, based on 
grammatical (syntactical) usage. Generating these datasets can take
CPU-months. Once generated, the can be used as a table lookup to obtain 
WSD info.

See the file "README-overview" for a more detailed motivational 
explanation of *why* this stuff is being done. See the README-example 
for an example of the WSD results in action.

Note: the Mihalcea research (PageRank on Semantic Networks, with 
Application to Word Sense Disambiguation Rada Mihalcea, Paul Tarau, 
Elizabeth Figa (2004)) shows that the Most Frequent Sense (MFS)
provides a very strong signal for the correct sense assignment. Thus,
the grammatical tags should improve on this.

The nlp/wsd directory contains an implementation of the Rada Mihalcea
word-sense disambiguation algorithm. The "stats-collection" directory 
contains a set of scripts that attach word senses to link-grammar parse 
disjuncts, and then update the corresponding scripts in SQL tables. 
SQL was used because of two reasons: 1) it can take months (or longer) 
to accumulate a sufficient amount of statistics to be meaningful.  
2) When the work was begun, it wasn't possible (and it wasn't clear how)
to store the accumulated data as a OpenCog hypergraph.

The directory "renorm-stats" contains code that implements the next steps
in this processing pipeline: scripts to compute marginal probabilities,
obtained from the raw counts stored in the SQL database.

The general analysis framework here is an outgrowth of the code that
is found in the 'lexical attraction' package, which is a distinct
launchpad project.  In particular, the SQL table definitions are given
there, not here. This cross-project tie-in is a bit of a mess at the
moment;  its not clear how to best resolve it.

Overview, Notes
---------------
The database contains counts for triples consisting of (sense, disjunct, word)
where 'sense' is a wordnet sense-key, obtained by the Mihalcea algo, 'disjunct'
is a link-grammar disjunct that was used to link a given word instance, and 
'word' is an word instance (as used in some sentence) in 'inflected' form (in
the form in which it appears in a link-grammar dictionary). The inflections
are typically '.v' for verbs, '.g' for gerunds, etc.

The probability of interest is p(s|d,w). If this is close to 1, this indicates
that a a certain word sense is being used almost uniquly/exclusively in a
certain context. By contrast, if this is close to zero, this indicates that 
the given sense is almost never used with the disjunct.

Also interesting is the entropy of a disjunct-word pair, summed over senses:
H(d,w) = - \sum_s p(s|d,w) log_2 p(s|d,w)

The basic idea is that low-entropy word-disjunct pairs imply a strong
clustering of disjuncts-to-senses.

rattle() package.
---------------

Preparing Link-Grammar Dictionaries
-----------------------------------
One of the primary applications of the WSD effort is to provide the Link 
Grammar parser with a set of dictionaries theat provide parse-ranking,
and provide parse-time syntactical word-sense disambiguation.  See the 
file renorm-stats/README for instructions on how to prepare these files.


Stand-alone scripts
-------------------
The following scripts are meant to be run stand-alone, as they do not
access any OpenCog atoms directly.  That is, they can run within the 
ordinary guile interpreter.

 * cluster.scm: Cheapo data clustering algo.

------------------------------------------------
Database: "lexat" -- i.e. use "psql lexat" to connect to it.

Misc notes:

   select * from disjunctsenses order by count desc;

   select * from disjuncts order by count desc;
   select * from disjuncts where log_cond_probability > 0.0 order by log_cond_probability asc;

Low entropy, but multiple senses observed says that there is a strong corellation
between disjunct and inflected word:
   select * from disjuncts where senses_observed>1 order by entropy asc;

Show low-entropy words, as well as the sense count; don't display boring columns:
   select disjuncts.inflected_word, disjuncts.disjunct, disjunctsenses.word_sense,
   disjuncts.entropy, disjuncts.senses_observed, disjunctsenses.count
   from disjuncts,disjunctsenses where
   disjuncts.senses_observed>1 and 
   disjuncts.inflected_word = disjunctsenses.inflected_word and
   disjuncts.disjunct = disjunctsenses.disjunct 
   order by disjuncts.entropy asc;


Suffers:
--------
Candidates:
  inflected_word   |    disjunct    |    word_sense     |       entropy       | senses_observed |        count         
-------------------+----------------+-------------------+---------------------+-----------------+----------------------
 suffers.v         | Ss- Os+ MVp+   | have%2:29:05::    | 0.00591198141479204 |               2 |     1.16565684723427
 suffers.v         | Ss- Os+ MVp+   | get%2:29:11::     | 0.00591198141479204 |               2 |   0.0005521501501815

Here, have%2:29:05:: is in the same synset as get%2:29:11:: (which is suffer%2:29:01::),

Since these both belong to the same synset, they should really be merged.
Done, the synset-renorm.pl script merges records that have the same synset.

Now its:
 suffers.v      | Ss- Os+ MVp+     | suffer%2:29:01:: |                0 |               1 |   1.16620899738445


She suffered a fracture in the accident"; 
"He suffered an insulin shock after eating three candy bars"; -- "He had an insulin shock after eating three candy bars"; 
"She suffered a bruise on her leg"; -- "She got a bruise on her leg"; 
"He got his arm broken in the scuffle"

This is clearly a different meaning than:
The new secretary had to suffer a lot of unprofessional remarks (which is I- Op+)
She was suffering after the accident,
She is suffering from the hot weather
She suffers from a tendency to talk too much (which is Ss- MVp+ MVp+, or MVa+, MVi+, etc.)
She suffered a terrible fate. (Ss- Os+)
This author really suffers in translation (Ss- MVp+)

Now, suffers.v is in words.v.2.2 which has the rules

/en/words/words.v.2.2: <verb-s> & <vc-tr,intr>;
<verb-s>: {@E-} & (Ss- or (RS- & Bs-));
<vc-tr,intr>: {O+ or B- or [[@MV+ & O*n+]]} & {@MV+};

The suffers.v | Ss- Os+ MVp+ disjuncts mean "transitive verb with a preposition",
and this seems to strongly indicate that the correct sense is *always* (suffer%2:29:01::)
for this sentence construction.

How about the past tense?

 suffered.v        | Ss- MVp+       | have%2:29:05::    |   0.0110064468007726 |               2 |      9.8004159256032
 suffered.v        | Ss- MVp+       | get%2:29:11::     |   0.0110064468007726 |               2 |   0.0094159056457721

Later, though, after more text processing:
 suffered.v     | Ss- MVp+                     | suffer%2:29:01::  | 0.383730502470793 |               4 |     24.2099497459028
 suffered.v     | Ss- MVp+                     | suffer%2:39:00::  | 0.383730502470793 |               4 |    0.788057528259323
 suffered.v     | Ss- MVp+                     | suffer%2:29:00::  | 0.383730502470793 |               4 |    0.454347413874991
 suffered.v     | Ss- MVp+                     | suffer%2:31:00::  | 0.383730502470793 |               4 |     0.17292852236826


she suffered a stroke   dj=Ss- Os+ 
 suffered.v     | Ss- Os+                      | suffer%2:29:01::  | 0.158424688370868 |               4 |     22.4992459030187
 suffered.v     | Ss- Os+                      | suffer%2:31:00::  | 0.158424688370868 |               4 |    0.463915849183753
 suffered.v     | Ss- Os+                      | suffer%2:37:00::  | 0.158424688370868 |               4 |    0.019114227845194
 suffered.v     | Ss- Os+                      | suffer%2:39:00::  | 0.158424688370868 |               4 |   0.0117731769456313

Clearly,  suffer%2:29:01:: is a big winner accross the board.

Also:

 suffers.v      | Ss- Os+ MVp+                  |   6.29677110165358 |
4.70087794722605 |                 |                 0 |               1
 suffers.v      | Ss- E- Os+ MVp+               |   3.12097296118736 |
5.45552526908809 |                 |                 0 |               1
 suffers.v      | Ss- MVa+ MVs+                 |   3.07256299257278 |
5.47807853897718 |                 |                 0 |               1
 suffers.v      | Ss- E- MVp+                   |   10.7104850448668 |
3.90891863346821 |                 |                 0 |               1
 suffers.v      | Ss- Ss- E- MVp+               |   0.72284644842148 |
7.56576000898761 |                 |                 0 |               1


Other Candidates
----------------
Verbs:
 gave.v            | Ss- Os+ MVp+ MVp+        | give%2:36:02::    |  0.00294323663814882 |               2 |     1.09565752530025
 gave.v            | Ss- Os+ MVp+ MVp+        | give%2:40:00::    |  0.00294323663814882 |               2 | 0.000236846846509587

 proposed.v        | Mv- MVp+ MVp+            | propose%2:32:00:: |   0.0050631354101759 |               2 |     1.19945158244393
 proposed.v        | Mv- MVp+ MVp+            | propose%2:32:01:: |   0.0050631354101759 |               2 | 0.000476899022006969

 ends.v            | Ss- MVp+                 | end%2:42:00::     |    0.011789696275097 |               2 |     6.02102715971805
 ends.v            | Ss- MVp+                 | end%2:42:01::     |    0.011789696275097 |               2 |  0.00625895238185481

Nouns:
 dog.n             | AN+                      | dog%1:05:00::     |  0.00512254376849792 |               2 |     1.10869675890655
 dog.n             | AN+                      | dog%1:06:00::     |  0.00512254376849792 |               2 | 0.000446655405590204

 towns.n           | Jp- Dmc-                 | town%1:15:00::    |  0.00717972910706398 |               2 |    0.802619180446346
 towns.n           | Jp- Dmc-                 | town%1:14:00::    |  0.00717972910706398 |               2 |  0.00047381451237468

 building.n        | Wd- Ds- Ss+              | building%1:06:00::|  0.00901884557840178 |               2 |     1.82290246346794
 building.n        | Wd- Ds- Ss+              | building%1:04:00::|  0.00901884557840178 |               2 |  0.00139490596324197

 home.n            | Cs- D*u- Ss+             | home%1:14:00::    |     0.01489989871691 |               2 |     1.14445717825025
 home.n            | Cs- D*u- Ss+             | home%1:06:02::    |     0.01489989871691 |               2 |  0.00155725637510752

Adjectives, adverbs, etc:
British.a

 personal.a        | A+                       | personal%3:00:00::|   0.0105270084251467 |               2 |     2.05028355814664
 personal.a        | A+                       | personal%3:01:01::|   0.0105270084251467 |               2 |   0.0018719470709944

 later             | E+                       | later%4:02:01::   |   0.0158438709016617 |               2 |      9.3855856634763
 later             | E+                       | later%4:02:03::   |   0.0158438709016617 |               2 |   0.0137091418558027

 however.e         | CO+                      | however%4:02:00:: |   0.0251996594250165 |               2 |     1.96656900377359
 however.e         | CO+                      | however%4:02:02:: |   0.0251996594250165 |               2 |  0.00492595230112824

 after             | Mgp+ Xc+ CO+             | after%4:02:01::   |    0.051442197937953 |               2 |    0.892771967980748
 after             | Mgp+ Xc+ CO+             | after%4:02:00::   |    0.051442197937953 |               2 |  0.00520906900520265

 new.a             | A+                       | new%3:00:00::     |   0.0579363661878467 |               2 |     47.9283157543877
 new.a             | A+                       | new%3:00:09::     |   0.0579363661878467 |               2 |    0.321947934003109


Lets look at the verbs
 gave.v            | Ss- Os+ MVp+ MVp+        | give%2:36:02::    |  0.00294323663814882 |               2 |     1.09565752530025
 gave.v            | Ss- Os+ MVp+ MVp+        | give%2:40:00::    |  0.00294323663814882 |               2 | 0.000236846846509587

Example for this disjunct would be:

 She gave a party for the orphans in the orphanage.

which has an object and two prepositions:

       +----------------MVp---------------+           
       +------MVp------+                  |           
       +---Os---+      +-----Jp----+      +----Js----+
 +--Ss-+   +-Ds-+      |    +--Dmc-+      |  +---Ds--+
 |     |   |    |      |    |      |      |  |       |
she gave.v a party.n for.p the orphans.n in the orphanage.n 

The sense give%2:36:02:: is (organize or be responsible for; "hold a
reception"; "have, throw, or make a party"; "give a course")

while give%2:40:00:: is (transfer possession of something concrete or
abstract to somebody; "I gave her my money"; "can you give me lessons?";
"She gave the children lots of love and tender loving care")

It seems hard to think of a sentance with an object and two preps
that would have any other sense for 'give'.

=======================
Background:

linkparser> She gave a party for the orphans
Found 2 linkages (2 had no P.P. violations)
	Linkage 1, cost vector = (CORP=3.7867 UNUSED=0 DIS=0 AND=0 LEN=11)

       +------MVp------+            
       +---Os---+      +-----Jp----+
 +--Ss-+   +-Ds-+      |    +--Dmc-+
 |     |   |    |      |    |      |
she gave.v a party.n for.p the orphans.n 


lexat=# select * from disjunctsenses where inflected_word='gave.v' and disjunct='Ss- Os+ MVp+ ' order by count desc;
   word_sense   | inflected_word |   disjunct    |       count        | log_cond_probability 
----------------+----------------+---------------+--------------------+----------------------
 give%2:36:02:: | gave.v         | Ss- Os+ MVp+  |     2.170205084805 |      1.5718979043102
 give%2:40:01:: | gave.v         | Ss- Os+ MVp+  |   2.05088588246407 |     1.65348207028009
 give%2:40:00:: | gave.v         | Ss- Os+ MVp+  |   1.50420148137325 |     2.10073146557915
 give%2:32:03:: | gave.v         | Ss- Os+ MVp+  |  0.387473482169565 |     4.05755980437411
 give%2:36:01:: | gave.v         | Ss- Os+ MVp+  |  0.262765684485569 |     4.61788050203826
 give%2:40:15:: | gave.v         | Ss- Os+ MVp+  | 0.0588864375564308 |     6.77565008049073
 give%2:40:05:: | gave.v         | Ss- Os+ MVp+  | 0.0175052491366771 |     8.52579788364965
(7 rows)

Hmm. give%2:40:01:: is (give as a present; make a gift of; "What will you give her for her birthday?")
while the remaining senses should indeed be considered to be "unlikely".

give (give%2:40:00::) (transfer possession of something concrete or abstract to somebody) "I gave her my money"; "can you give me lessons?"; "She gave
the children lots of love and tender loving care"

After more processing, the top five are now:
 give%2:36:02:: | gave.v         | Ss- Os+ MVp+  |   9.19687218041551 |    0.971479113216472
 give%2:40:00:: | gave.v         | Ss- Os+ MVp+  |   3.01082867642231 |     2.58246178547495
 give%2:40:01:: | gave.v         | Ss- Os+ MVp+  |   2.79092000659383 |      2.6918816285251
 give%2:40:06:: | gave.v         | Ss- Os+ MVp+  |   1.20031466833253 |     3.90920973802285
 give%2:40:09:: | gave.v         | Ss- Os+ MVp+  |   1.09324581118626 |     4.04400458227927

Notice the gap between top sense, and the rest, has widened.


	Linkage 2, cost vector = (CORP=4.4040 UNUSED=0 DIS=1 AND=0 LEN=9)

       +---Os---+      +-----Jp----+
 +--Ss-+   +-Ds-+--Mp--+    +--Dmc-+
 |     |   |    |      |    |      |
she gave.v a party.n for.p the orphans.n 

lexat=# select * from disjunctsenses where inflected_word='gave.v' and disjunct='Ss- Os+ ' order by count desc;
   word_sense   | inflected_word | disjunct |        count        | log_cond_probability 
----------------+----------------+----------+---------------------+----------------------
 give%2:36:02:: | gave.v         | Ss- Os+  |     5.5046642296017 |    0.790359680824649
 give%2:40:01:: | gave.v         | Ss- Os+  |    1.32115735311106 |      2.8492119412325
 give%2:40:00:: | gave.v         | Ss- Os+  |   0.844711858956604 |      3.4944830362432
 give%2:40:06:: | gave.v         | Ss- Os+  |   0.644499839101934 |     3.88476234306743
 give%2:36:01:: | gave.v         | Ss- Os+  |   0.547392147160068 |     4.12036760341426
 give%2:40:03:: | gave.v         | Ss- Os+  |   0.339905223753224 |     4.80780980772028
 give%2:36:00:: | gave.v         | Ss- Os+  |   0.235391632520718 |     5.33787930376052
 give%2:38:13:: | gave.v         | Ss- Os+  |  0.0567291219804709 |     7.39078090136801
 give%2:32:03:: | gave.v         | Ss- Os+  |  0.0199347850664697 |     8.89958238585429
 give%2:34:00:: | gave.v         | Ss- Os+  | 0.00596139208782609 |     10.6411492670705
(10 rows)

Not at odds with the above.

After more stats, the top four are now:
 give%2:36:02:: | gave.v         | Ss- Os+  |    18.0263471677313 |    0.351379216004031
 give%2:40:01:: | gave.v         | Ss- Os+  |    1.75019171085476 |     3.71590143240663
 give%2:40:00:: | gave.v         | Ss- Os+  |    1.24343275701667 |     4.20908590081861
 give%2:40:06:: | gave.v         | Ss- Os+  |   0.654998252433946 |     5.13385142910354

Again, the gap has widened!

Can we disprove the above?

linkparser> John  gave him instructions on how to use the tool
Found 2 linkages (2 had no P.P. violations)
	Linkage 1, cost vector = (CORP=17.0000 UNUSED=0 DIS=2 AND=0 LEN=14)

          +----------MVp----------+                      
          +------Opn-----+        |          +----Os----+
   +--Ss--+-Ox-+         |        +QI+TOn+-I-+    +--Ds-+
   |      |    |         |        |  |   |   |    |     |
John.b gave.v him instructions.n on how to use.v the tool.n 

select * from disjunctsenses where inflected_word='gave.v' and disjunct='Ss- Ox+ Opn+ MVp+ ' order by count desc;
 word_sense | inflected_word | disjunct | count | log_cond_probability 
------------+----------------+----------+-------+----------------------
(0 rows)

Still zero, after more stats.

	Linkage 2, cost vector = (CORP=17.0000 UNUSED=0 DIS=3 AND=0 LEN=12)

          +------Opn-----+                   +----Os----+
   +--Ss--+-Ox-+         +---Mp---+QI+TOn+-I-+    +--Ds-+
   |      |    |         |        |  |   |   |    |     |
John.b gave.v him instructions.n on how to use.v the tool.n 

select * from disjunctsenses where inflected_word='gave.v' and disjunct='Ss- Ox+ Opn+ ' order by count desc;
 word_sense | inflected_word | disjunct | count | log_cond_probability 
------------+----------------+----------+-------+----------------------
(0 rows)

Still zero.

Try alternate:

Found 2 linkages (2 had no P.P. violations)
	Linkage 1, cost vector = (CORP=17.0000 UNUSED=0 DIS=2 AND=0 LEN=14)

        +----------MVp----------+                      
        +------Opn-----+        |          +----Os----+
  +--Sp-+-Ox-+         |        +QI+TOn+-I-+    +--Ds-+
  |     |    |         |        |  |   |   |    |     |
they gave.v him instructions.n on how to use.v the tool.n 

select * from disjunctsenses where inflected_word='gave.v' and disjunct='Sp- Ox+ Opn+ MVp+ ' order by count desc;
Nothing so far -- zero rows

linkparser> 
	Linkage 2, cost vector = (CORP=17.0000 UNUSED=0 DIS=3 AND=0 LEN=12)

        +------Opn-----+                   +----Os----+
  +--Sp-+-Ox-+         +---Mp---+QI+TOn+-I-+    +--Ds-+
  |     |    |         |        |  |   |   |    |     |
they gave.v him instructions.n on how to use.v the tool.n 

select * from disjunctsenses where inflected_word='gave.v' and disjunct='Sp- Ox+ Opn+ ' order by count desc;
   word_sense   | inflected_word |   disjunct    |       count       | log_cond_probability 
----------------+----------------+---------------+-------------------+----------------------
 give%2:36:02:: | gave.v         | Sp- Ox+ Opn+  |  1.95710706403113 |    0.427502836651755
 give%2:40:00:: | gave.v         | Sp- Ox+ Opn+  | 0.348671086794809 |     2.91628687764095
 give%2:40:01:: | gave.v         | Sp- Ox+ Opn+  | 0.210736996086307 |     3.64271000289082
 give%2:40:06:: | gave.v         | Sp- Ox+ Opn+  |  0.11560531617747 |     4.50894587012378

The sense give%2:36:02:: is (organize or be responsible for; "hold a
reception"; "have, throw, or make a party"; "give a course")

This time, the choosen sense is incorect; might have expected:
give (give%2:40:00::) (transfer possession of something concrete or abstract to somebody) 
"I gave her my money"; "can you give me lessons?"; 
"She gave the children lots of love and tender loving care"

But ... also .. but maybe not ... ??
 give (give%2:40:07::) (convey or reveal information) "Give one's name"
 give (give%2:40:09::), (transmit (knowledge or skills)) "give a secret
to the Russians"; "leave your name and address here"; "impart a new skill to the students"

However 'give' is a very nuanced word; wordnet lists 43 verb senses for it!!

========================================================================
========================================================================
========================================================================
OK, how about syntactic word clustering? Link-grammar organizes words
into those that have similar rule sets. Can we find and correct 
(or refine) these clusters? Some words are in too narrow a cluster,
others might be in too wide a set.  Want to refine both membership
classes.  In particular, want to widen clusters (to allow more sentences
to be parsed) also want to refine clusters (so that nly tryuly 'similar'
words are in a set. So -- lets try by hand ...

In the disjuncts table, log_cond_probability is minus the log_2 of
the conditional probabilty of seeing this disjunct, given this 
inflected word.

select * from disjuncts where disjunct='Ss- Ox+ Opn+ ' order by log_cond_probability asc;


 inflected_word |   disjunct    |       count        | log_cond_probability |  
----------------+---------------+--------------------+----------------------+-
 denies.v       | Ss- Ox+ Opn+  |   2.70718437433243 |     3.09395696205239 |
 owes.v         | Ss- Ox+ Opn+  |     1.766691416502 |     3.39584024085858 |
 buys.v         | Ss- Ox+ Opn+  |   1.08895570039749 |     3.76726502765287 |
 gave.v         | Ss- Ox+ Opn+  |   15.2900903802364 |      4.6935850263336 | 
 teaches.v      | Ss- Ox+ Opn+  |  0.881453424692153 |     5.04181655775547 | 
 gives.v        | Ss- Ox+ Opn+  |   5.32094312645492 |     5.59212083110965 | 
 finds.v        | Ss- Ox+ Opn+  |   1.47437614202499 |     5.71456136184912 | 
 refuses.v      | Ss- Ox+ Opn+  |  0.459859251976013 |     5.75651750326265 | 
 read.v         | Ss- Ox+ Opn+  |   1.83886253833771 |     5.82941333216396 | 
 asked.v        | Ss- Ox+ Opn+  |   1.79761986061931 |     5.87878507916957 | 
 taught.v       | Ss- Ox+ Opn+  |   3.70534968376159 |     5.99382568059538 | 
 granted.v      | Ss- Ox+ Opn+  |    4.0402828156948 |     6.07583566445166 | 
 asks.v         | Ss- Ox+ Opn+  |   1.18346454203128 |     6.08104628882818 | 
 shows.v        | Ss- Ox+ Opn+  |   1.92236000299453 |     6.27355627951066 | 
 brought.v      | Ss- Ox+ Opn+  |      2.43976123631 |     6.54762703580705 | 
 gets.v         | Ss- Ox+ Opn+  |  0.606530666351318 |     6.70894786903452 | 
 appointed.v    | Ss- Ox+ Opn+  |  0.910481497645378 |     7.25544095246871 | 
 made.v         | Ss- Ox+ Opn+  |   3.21262238919736 |     7.73259168120915 | 
 wrote.v        | Ss- Ox+ Opn+  |  0.422241305932403 |     7.90307163730819 | 

No visually obvious clustering.

select * from disjuncts where disjunct='Ss- Ox+ ' and count > 1.0 order by log_cond_probability asc;

   inflected_word   | disjunct |      count       | log_cond_probability |  
--------------------+----------+------------------+----------------------+--
 harpoons.v         | Ss- Ox+  | 1.23413008451462 |                    0 | 
 blacklists.v       | Ss- Ox+  | 1.21693927049637 |                    0 | 
 deserts.v          | Ss- Ox+  | 1.87540191411972 |                    0 | 
 brainbustered[!].v | Ss- Ox+  | 1.38029098510743 |                    0 | 
 overwhelms.v       | Ss- Ox+  |  3.7728200852871 |  0.00388966262389523 | 
 assaults.v         | Ss- Ox+  | 2.15775206685066 |    0.406605322094401 | 
 coddles.v          | Ss- Ox+  | 1.91823095083236 |      0.4896415709171 | 
 pressurises.v      | Ss- Ox+  | 1.05867084860802 |    0.554480230164364 | 
 legitimised.v      | Ss- Ox+  | 1.09617540240288 |    0.577028231407784 | 
 hugs.v             | Ss- Ox+  | 1.14638306200504 |    0.689778113607922 | 
 discards.v         | Ss- Ox+  | 3.46550455689431 |    0.833836762428404 | 
 muffed.v           | Ss- Ox+  |  1.1338666677475 |    0.954090158106465 | 
 punishes.v         | Ss- Ox+  | 4.61573058366775 |    0.954881042038653 | 
 bashes.v           | Ss- Ox+  | 1.08810368180275 |    0.964721274701267 | 
 inconvenienced.v   | Ss- Ox+  | 1.17411637306213 |     1.02200334909295 | 
 fumbles.v          | Ss- Ox+  | 1.06313192844391 |     1.05022079574155 | 
 teases.v           | Ss- Ox+  |  5.4783154539764 |     1.13421908343243 | 
 corners.v          | Ss- Ox+  | 1.79577225446701 |     1.15891817885022 | 
 rapes.v            | Ss- Ox+  | 5.62726438790561 |     1.16145451305937 | 
 handcuffs.v        | Ss- Ox+  | 1.69892004132271 |     1.18468837013388 | 
 blesses.v          | Ss- Ox+  | 2.77992057800294 |     1.22104640419092 | 


No obvious clustering either.

What sort of meteric to assign?
1) only if count > 1.0
2) only if log_cond_prob < 5
3) each dj on own axis.
4) if dj not present, distance=5 (i.e. cutoff)

The above is implemented in cluster.scm

 SELECT COUNT(*) FROM disjuncts where count > 1.0;
The disjuncts table has 1.35M entries, so computing a matrix is infeasible.

SELECT COUNT(*) FROM disjuncts where count > 1.0 and log_cond_probability<5;
911K rows when clipped.


Some prelim results:
 parliamentary.a democratic.a statutory.a part-time.a chief.a proprietary.a eHow[?].a anonymous.a
personalized.v radioactive.a biological.a primary.a rural.a contemporary.a 404.4/sq[?].a exact.a
widepread[?].a symbolic.a 19th[?].a belching.v Spanish.a daily.a canine.a juvenile.a total.a blazing.v
midwinter[?].a incredible.a near.a soft.a ignimbrite[?].a propagandistic[?].a Japanese.a pg[?].a
treacherous.a approaching.g ill-fated.a eikonal[?].a differential.a computational.a medieval.a
allegorical.a og[?].a principal.a cardinal.a goldmining[!].g free.a heavenly.a moral.a relict[?].a pagan.a
ritual.a potential.a geographical.a mythological.a northwestern.a natural.a advancing.g para[?].a ornate.a
coastal.a sigmoidal[?].a formal.a 2001/02[?].a da[?].a electoral.a Irish.a joint.a electrical.a
particular.a normal.a technical.a bassist[?].a)

Almost all are already in words.adj.1, except for:
words.v.1.4:belching.v
words.v.1.4:blazing.v
words.v.2.5:advancing.g
words.v.2.5:approaching.g

which indicates that these gerunds are used almost always as adjectives, and not something else.
Notice that link-grammar correctly guessed "widpread" (a typo), sigmoidal and eikonal, although
it flopped on bassist.


Stats:
------
53653 articles in the coged directory (53.6K wikipedia articles, in 
the latters A-E range.)  How many senteces? Try grep:
    grep "<sentence index=" */* |wc 
1186858 sentences (1.2M) or 22.121 sentences per article
    grep "<parse id=" |wc
3383657 parses: (3.4M) or 2.85094 parses per sentence.

weighted parse score = 991235 (991K)
= sum exp-(0.012*len+0.06*disj + 0.2*and + 0.4*skipped)
= 0.292948 per parse = 0.29*2.85 = 0.835178 per sentence

word-count = 76129120 = 76M = 22.5 words per sentence.
weighted word-count = 17665277 = 17.7M = 0.2320436 per word

Above is what we *should* have gotten ... except that a portion of the
early test runs were discarded due to bad data/bugs/flawed runs. So of 
the 17M, here's what actually ended up in the database:

In the lexat database: (psql lexat)
select * from unigramcount; --- 13436741 (13.4M)
select * from lemmacount; -- 12936465 (12.9M)

13.4M/17.7M= 76% of the data was kept.  Good.

So, defacto, 3/4ths of 53653 articles were used, etc.

But wait -- how many were taged with disjuncts?

select sum(count) from disjuncts;  -- 23444129.  i.e. 23M !! ??
select sum(count) from disjunctsenses; -- 593003 i.e 593K !!??

I'm getting confused a bit here ... 
Did I pump some huge amount of data into the disjuncts table, 
but not the disjunct-senses table ?? Or did the mihalcea algo
fail to normalize properly, and knock down the counts?

How many unique words observed?
-------------------------------
select count(*) from unigram; 230018 i.e. 230K words
select count(*) from unigram where count > 1.0; 116628 i.e. 117K

This includes foreign words, numbers.

select count(distinct inflected_word) from disjuncts; -- 953386 -- i.e. 953K wow!!
select count(distinct inflected_word) from disjuncts where count > 1.0; -- 330520 -- 330K

select count(distinct inflected_word) from disjunctsenses; -- 15194 -- 15K
select count(distinct inflected_word) from disjunctsenses where count > 1.0; -- 4788 -- 5K


How many disjuncts observed?
----------------------------
(redo previous analysis in now 2009)
SELECT COUNT(*) FROM disjuncts; 5666488 -- i.e. 5.6M word-disjunct pairs.

SELECT COUNT(*) FROM disjuncts where count > 1.0; 1528731 -- i.e. 1.5M pairs above thresh.

SELECT COUNT(*) FROM disjuncts where count > 1.0 and log_cond_probability<5; 
685953 i.e. 686K

SELECT COUNT(DISTINCT disjunct) FROM disjuncts;
394832 i.e. 395K  ?? wtf ?? wow .. this is really correct! -- verified
by dumping all data into file then sort | uniq |wc to count lines in file.

OK. but wait ... 
SELECT COUNT(DISTINCT disjunct) FROM disjuncts where count > 1.0;  
gives a MUCH lower number: 83644 -- i.e. 83.6K wow!

SELECT COUNT(DISTINCT disjunct) FROM disjunctsenses;
gives 46490 -- i.e. 46.5K wow whoa wtf ... 

SELECT COUNT(DISTINCT disjunct) FROM disjunctsenses where count > 1.0;
gives 8940 -- i.e. 9K wowww

Of these 46.5K dj's there are 588 connectors, 430 link types, of which
45 were idiom links.

WTF major links: disjunctsenses has 89, but disjuncts have 106 !!!
The following were not observed in disjunctsenses:
AA -- occur only in questions
AM -- as many
ER -- the better the more 
EZ -- almost as idiomatics
FL -- for long idiom
FM -- from prep
H -- how many much questions
HA -- how many 
JQ -- prep question
LE -- comparatives
MF -- idiomatic
NF -- two third of a
NT -- not to
NW -- two thirds
TR -- comparatives
UN -- until
WR -- where queston



How many senses were observed?
------------------------------
SELECT COUNT(DISTINCT word_sense) FROM disjunctsenses;
19704 -- 19.7K


How many unique word-disjunct pairs?
-------------------------------------
 SELECT distinct on (inflected_word, disjunct) * INTO xx FROM disjunctsenses;
 SELECT count(*) from xx;
 237012 = 237K

SELECT inflected_word, disjunct, count into xx FROM disjunctsenses;
CREATE INDEX xxi on xx (inflected_word, disjunct);
SELECT inflected_word, disjunct, SUM(count) AS sum_count into xy FROM xx GROUP BY inflected_word, disjunct;
SELECT COUNT(*) FROM xy;
SELECT COUNT(*) FROM xy WHERE sum_count > 1.0;

the last says: 56605 -- 56.6K had significant counts.


How many word-disjunct-senst triples?
-------------------------------------
SELECT COUNT(*) FROM disjunctsenses;
 642776 -- 643K

SELECT COUNT(*) FROM disjunctsenses where count > 1.0;
78128 -- 78K

Entropy distribution
--------------------
Total count is 593003.630924276 for 537985 items
Done updating the entropy of 226679 disjuncts
Avg sense cnt=2.37333409799761 avg entropy=0.777430272279844;

SELECT * FROM Disjuncts WHERE entropy >= 1.0 and entropy < 1.01333333333333;
 word      | disjunct  |     count           |     entropy      | senses_observed 
 wish.v    | Sp- TO+   |    42.1725730542092 | 1.01283940271225 | 4
Court      | AN+       |    132.541593157099 |                1 | 2
electricity.n  | AN+   |    140.905140136138 | .999650369735075 | 2
hit.n      | AN+       |    330.162172231242 | .996723422284469 | 3
particular.n  | AN+    |     794.61839278614 | .991380485748673 | 2



      word_sense       | inflected_word | disjunct |      count       | log_cond_probability 
-----------------------+----------------+----------+------------------+----------------------
 electricity%1:19:01:: | electricity.n  | AN+      | 2.85156721992663 |    0.968583915135987
 electricity%1:19:00:: | electricity.n  | AN+      | 2.72871836278425 |     1.03211545839439
(2 rows)

lexat=# SELECT * FROM DisjunctSenses WHERE inflected_word='hit.n' and disjunct='AN+ ';
  word_sense   | inflected_word | disjunct |       count        | log_cond_probability 
---------------+----------------+----------+--------------------+----------------------
 hit%1:04:03:: | hit.n          | AN+      |  0.785423368973435 |     0.39030935108556
 hit%1:04:00:: | hit.n          | AN+      |  0.172659081097037 |     2.57585365922672
 hit%1:11:01:: | hit.n          | AN+      | 0.0713518655147212 |     3.85075681975652
(3 rows)



TODO
----
sense-tagged triples (word, disjunct,sense) table needs to be redone,
due to following problems:

-- Current database has bad/broken entropy distribution on these triples.
   This is because there were islands in the graph algo, which resulted
   in equal-weight sense tags to be stored. These cause huge artificial
   bumps in the entropy-vs (word,disjunct) distribution.
-- Islands can be tagged using most-frequent sense. (MFS)
   (Alternately, islands can be removed entirely -- this will be the 
   final strategy -- and is now done in Sweep.cc)
-- Accuracy can be improved by weighting with most-frequent sense.
   In fact, MFS seems to provide a very strong WSD signal.
   (This will not be used for tagging).
-- Recent link-grammar changes have changed subscripts, making old 
   table obsolete. 
-- Should not record triples from sentences which have unused words
   in the parse. Current plan is to avoid these by assigning a parse
   rank of 0.0 to these sentences -- this seems like the easiest 
   solution. Its currently implemented in the relex script
   relex/src/perl/cff-to-opencog.pl
-- Need to make sure the wsd is using only one word per sysnet ....
   I think this is OK right now, .. but having two from the same 
   synset dilutes the strength going to the nodes and wrecks the data.
-- Also, maybe need to tweak the sense-similarity scores?  i.e. are
   the sense-similarity scores invariant under synsets? That this 
   is correctly done/stored is not obvious ...

-- See the draft journal paper for more info, see also the lexat
   project and the src/lexat/senseval/README file for a complimentary 
   take on this.
