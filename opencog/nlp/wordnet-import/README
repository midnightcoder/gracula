
                     WordNet Import
                     --------------
                 Linas Vepstas April 2008

Import the WordNet synsets into opencog.  

The WordNet synset importer scans the wordnet data files, and prints
NMXML or scheme to stdout. The output can be piped sent to a file, 
or piped directly to the opencog server.

To dump to a file, run, at the command-line, ./wni >file
To pipe directly to the opencog server, ./wni | telnet localhost 17001

The choice of scheme of NMXML output is a compile-time option.
Another compile-time option is the printing of the sense-keys only,
or sense-keys plus relations. 

During dumping, there are many warnings printed to stderr, for certain 
unhandled relations. These can be safely ignored (and used to gauge 
progress).

The importer assumes a default location for the wordnet sense.index
file (in /usr/share/wordnet). A different location can be specified 
on the command-line.

Building the importer requires wordnet to be installed; it makes
use of the wordnet programming API.

As of May 2008, the importer appears to work well, generating the 
XML markup explained below.  The generated XML is suitable for 
current needs, but may evolves as requirements change. For example,
the importer does not handle all relationships; in particular, it
ignores "pertaining to", "caused by" and "entails" relationships.

The wordnet database contains about 207 thousand word senses, 
arranged into 37MB of flat-ASCII files; this works out to about
180 Bytes per word-sense.

Currently, dumping to a file results in a 630MB file. This works
out to about 3KB of XML per word-sense -- no surprise; XML is 
verbose.

It takes about 15 minutes (on current CPU's) to dump, most of the 
time spent in the wordnet code (as opposed to opencog code). 

The opencog server will grow to about 2.6 gigabytes! while reading
the full dataset. This works out to about 12KB of opencog atoms per
word sense.

To reduce mem usage, the "skip_collocations" flag was added to skip
over all collocations in wordnet. There appear to be about 120K 
word senses that are not collocations. For Intel 32-bit mode, sizes
as follows:

-- Loading these into opencog resulted in 1.04GBytes of RAM usage,
   or 8.7KB of opencog atoms per word sense. This is with TLB 
   lookup tables (used only for the SQL backend).

-- Turning off TLB lookup, the load uses 670MBytes of RAM, or about
   5.6KB per word sense.  Thus, the TLB lookup tables add a hefty 55%
   storage overhead.

-- AtomTable.cc defines USE_ATOM_HASH_SET to improve table performance.
   Turning it off increases CPU usage by 100-fold or more, with no
   appreciable memory savings. Long runs end with memory corruption.

-- Updating AtomSpace/Trail.cc with new version from Welter Luigi 
   decreased RAM usage to 207MBytes, or 1.7KB per word sense. Wow!

   Similarly, the full (with collocations) load yields: 399MB/187K
   = 2.1KB per word sense.


Design Overview
---------------

Choice of programming API
-------------------------
There does not appear to be any "adequate" programming interface to
wordnet! There are many many interfaces, and yet, none of them are
very good, much less comprehensive!

The native wordnet interface is in the "C" programming language.
It does not seem to be a well-designed interface. It does not seem to
support any means by which the entire contents of the database can be
dumped. It has serious problems:
-- some routines, such as GetSenseIndex, do bad mem references.
-- some data fields, such as ppos, contain garbage data
-- the synset structure fails to contain enough info on its own, 
	to print related words (hypernyms, etc).
-- the is_defined() routine is less than accurate in its return values.
-- the code uses inconsistent naming conventions.
-- the api is incomplete: there are no routines for building up 
   a sense-key.

However, the C api is very well-documented; which makes it superior
to all other systems.

The use of Python, and the NLTK wordnet module was briefly explored.
Unfortunately, the NLTK wordnet interfaces lack support for wordnet 
sense-keys, which is crucial for properly/easily cross-indexing the
various relations. The NLTK python interface is undocumented.

The perl interface WordNet::QueryData does not seem to support
sense-keys either.

The perl interface Lingua-Wordnet does not seem to support wordnet-3.0.

The Java interface JWNL does not seem to support wordnet-3.0, and 
the API appears to be under-documented.


Wordnet-to-Opencog Mapping
--------------------------
The section proposes a mapping to OpenCog pseudo-XML. Actual OpenCog
XML has a slightly more complex structure, and is harder to read. 
Details for the actual XML format are given in a later section.

Word senses
-----------
Associated with every word is a list of possible senses.
Consider the word "bark", thus:

    <WordNode name="bark" />   -- a word.

Wordnet entries are not necessarily single words; more properly, the
should be called "lexical units": they can be single words,
collocations, idioms, and so on.

A "word sense" is a collection of information about a single semantic
sense of a word. This bag includes information about the part-of-speech
(noun, verb, etc.), example usages, pointers to synonym sets (synsets),
hyponyms, hypernyms, etc. A specific "word sense" will be tagged with a
unique identifier, which is then used to reference that sense.

In the OpenCog type hierarchy, (type.script)
   WORD_SENSE_LINK <- ASSOCIATIVE_LINK

A WordSenseLink couples the tag that will stand for the word sense,
to the word itself:

   <WordSenseLink>
      <WordNode name="bark" />
      <ConceptNode name="bark_sense_23" />
   </WordSenseLink>

The tag value "bark_sense_23" has no particular meaning in itself,
it is just some unique string used to identify the sense.

Word senses can be crudely categorized according to part-of-speech.
Thus, for example:

   <PartOfSpeechLink>
      <ConceptNode name="bark_sense_23" />
      <PartOfSpeechNode name="noun" />
   </PartOfSpeechLink>

The above introduces a new node type "PartOfSpeechNode", and a new link
type: "PartOfSpeechLink".  Other possible properties might include gender,
entity tags, and so on. This potentially leads to an explosion of 
special-purpose nodes. For example, there are over a dozen properties
that RelEx uses to tag words.  To avoid this explosion, the following 
syntax will be used:

   <EvaluationLink>
     <PredicateNode name="PartOfSpeech"/>
     <ListLink>
       <ConceptNode name="bark_sense_23" />
       <ConceptNode name="noun" />
     </ListLink>
   </EvaluationLink>

[[Square brackets set off design discussions; below follows some
commentary about alternate ways in which the above might have been
achieved.

The above specifies a triple that is to be associated together, the
triple ("bark_sense_23", PartOfSpeech, "noun").  Another possible way
of denoting a triple might have been:

   <PropertyLink>
     <ConceptNode name="bark_sense_23" />
     <ListLink>
       <ConceptNode name="PartOfSpeech"/>
       <ConceptNode name="noun" />
     </ListLink>
   </PropertyLink>

but this would require the invention of a new link type, "PropertyLink",
which other parts of PLN are unaware of -- PLN already has a fairly
established set of relations it is used to handling. Alternately, 

   <MemberLink>
     <ListLink>
       <ConceptNode name="PartOfSpeech"/>
       <ConceptNode name="noun" />
     </ListLink>
     <ConceptNode name="bark_sense_23" />
   </MemberLink>

could be useful, but this would loose the benefit of the predicate form.
A predicate could still be created later, by making use of a link from
the predicate name to a SatisfyingSetLink. ]]

[[Per Ben, it seems desirable to *define* a new, unknown link type L to
be
    L A B
    =
    EvaluationLink
       L
       ListLink A B

Thus, the new link type PartOfSpeechLink is understood to be
mathematically identical to an evaluation link where the predicate name
is "part of speech". ]]

Each of these nodes and links are assigned a truth value of 1 and a
confidence of 1. Non-unit truth-values & confidence will be used when 
relating word senses to the use of actual words in sentences. Here, 
however, the goal is to state "this word does have this as one possible
meaning, no matter how rarely used this meaning might be."

[[Some word senses are more common than others. For example, archaic
word senses tend to be rarely encountered ... except when one is 
reading an archaic text. This is addressed in a section below.]]

In WordNet, word senses are commonly established by giving "sister
terms" (a WordNet concept, equivalent to "synonyms for word senses")
as well as hypernyms, hyponyms, part holonyms, etc. A sister term,
hypernym, hyponym, etc. are thus links. Since some sister terms are 
closer than others, these links will have non-unit truth values. 

Hypernyms and hyponyms will be mapped to inheritance links.
A hypernym for the wood-bark is that it is a covering of something:

   <InheritanceLink strength=0.8 confidence=0.9 />
      <WordSenseNode name="bark_sense_23" />
      <WordSenseNode name="covering_sense_42" />
   </InheritanceLink>

A hyponym for wood-bark is that cork is a kind of bark:

   <InheritanceLink strength=0.8 confidence=0.9 />
      <WordSenseNode name="cork_sense_98" />
      <WordSenseNode name="bark_sense_23" />
   </InheritanceLink>

Note the reversed order between the hypo- and hyper- representations.

WordNet includes the idea of a holonym, namely, that something is a
part of something else, and its converse, the meronym, that something is
composed of parts. It furthermore subdivides these into three
subclasses: 
-- member holonym/meronym
-- substance holonym/meronym
-- part holonym/meronym

   HOLONYM_LINK <- INHERITANCE_LINK

Assigning all three subclasses to a single type of link does erase some
of the information offered by WordNet; this may need to be revisited.

To continue with the tree-bark example:

   <!-- bark is a part of a branch -->
   <HolonymLink strength=0.8 confidence=0.9>
      <WordSenseNode name="bark_sense_23" />
      <WordSenseNode name="branch_sense_2" />
   </HolonymLink>

   <!-- bark is composed of lignin -->
   <HolonymLink strength=0.8 confidence=0.9>
      <WordSenseNode name="lignin_sense_1" />
      <WordSenseNode name="bark_sense_23" />
   </HolonymLink>

There are distinct hypernym hierarchies for nouns and verbs;
these hierarchies do not touch. Only nouns have a holonym
hierarchy.

Nouns and verbs are related by means of "derivationally
related forms", e.g. verbized nouns, etc.

Adjectives and adverbs are not organized in hypernym or holonym 
hierarchies. Instead, they are grouped according to similarity.
(SIMPTR). The similarity relation is not used for nouns and verbs.
The similarity relation will be mapped with with SimilarityLinks

   SIMILARITY_LINK <- UNORDERED_LINK


WordNet as a number of other relations, including: 
troponyms,
sister terms,
antonyms, 
entailment (e.g. weaning entails nursing),
cause, 
participles, 
pertaining-to.

Actual NMXML details
--------------------
Nodes that are included inside of links have a different markup 
than free-standing nodes. Thus, the correct notation would be:

   <PartOfSpeechLink>
      <Element class="ConceptNode" name="bark_sense_23" />
      <Element class="PartOfSpeechNode" name="noun" />
   </PartOfSpeechLink>

Furthermore, new nodes must be specified outside of a link, before they
can be used inside of a link. That is, one must specify 

   <ConceptNode name="bark_sense_23" />
   <PartOfSpeechNode name="noun" />

before being able to use either inside a link.

Truth/confidence values are specified as follows:

   <SimilarityLink strength=0.8 confidence=0.9>


Performance
-----------
There are two types of output formats that can be generated: 
NMXML and Scheme.

wc on the NMXML output results in: 16350243 45025227 648664672
So NMXML requires 16M lines in 648 MBytes; this bzip2's down to 
a 16MB file.  By contrast, the scm output has a wc of 3307375 
14804016 189058447 i.e. 3.3M lines occupying 189 MBytes. This 
bzip2's down to 8.1 MBytes

load 186605 senses 
NMXML:  in 4:18 mins CPU on cogserver and 1.1g RAM usage
scheme: in 2:42 mins CPU on cogserver, and 417G RAM usage.

Can probably decrease the NMXML usage to same as scheme, by 
slowing down the load rate -- there's a bug, maybe a mem
leak, in the tcpip code. Slowing the rate of traffic on the
socket decreases the ram usage. See below for forensics,
or the wiki page/email for the bug report.

For the case of senses+POS only, this creates 973300 atoms.

scm: 690 MB in 45 seconds
XML: 359 MB in 71 seconds
SQL: 271 MB in 25 seconds

scm: 559 MB in 45 secs, after changing AtomTable to use 
onstack local variable allocation, instead of new/delete.
That saved 131 MB of fragmentation!  WOW!!!!

Now down to 451 MB
Now down to 271 MB

since the SQL loader is the cleanest, this strongly suggests 
severe memory fragmentation.

Some variants: let: 1.4GB in 5:21
               let*: 1.2GB in 1:55
               unique-words: 1.06 GB in 1:41

sensenode x 186605
poslink x 186605
wordnode x 137450
senselink x 462640

total 324055 height 0 = (186605 + 137450) nodes
total 649245 height 1 = (462640 + 186605) links
total 973300 atoms
