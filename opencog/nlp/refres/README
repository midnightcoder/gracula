
                         Reference Resolution
                         --------------------
                             October 2008
                             Linas Vepstas

The core problem of reference resolution is determining when two words
in a text refer to the same concept. Thus, for example, the same noun,
used in neighboring sentences, probably refers to the same concept.
Alternately, anaphora (he, she, it, etc.) may also be used to refer to
the same concept.

This README file contains some working notes on how reference resolution
might be accomplished within the current OpenCog infrastructure. These
notes may at times be incoherent or disconnnected; this is a
work-in-progress.

XXX The content here is out-of-date. Much of what used to be discussed
below has been given a concrete resolution in nlp/triples/README. The 
more abstract problem of reference resolution is dealt with in 
nlp/seme/README. This directory is an empty placeholder, waiting for an
implementation of the Hobbs algo.

Overview
========
The current plan is to perform reference resolution using a collection
of "rules" or "proceedures" to recognize likely co-referents, and then
to check for reference consistency using reasoning. An example of a set
of rules is the "Hobbs algorithm" for anaphora resolution.

The goal here is to implement all of these rules/algorithms as a set of
OpenCog hypergraphs. The reason for doing this is to allow the system to
automatically learn, reinforce, and modify these sets of rules.  Thus,
for example, the Hobbs algo has been implemented in Java, as a part of
the RelEx package. However, if changes are needed to this algorithm are
needed, then only a human programmer is capable of performing these
changes, as only a human programmer can (currently) understand Java and
make meaningful modifications to Java code.  By contrast, the goal here
is to perform a statistical analysis of text, and to automatically
discover such rules, as best as possible, assign them likelyhoods and
weights that correspond to thier suitability in any given linguistic
situation.

There are then four practical considerations to acheiving the above
program:
1) Understanding how to represent "rules" as OpenCog hypergraphs
2) Having a "rule engine" that can apply these rules.
3) Having a persistent database that can accumulate statistical
   information about the usage of these rules,
4) Having a "perception" (or "cluster analysis" or "feature extraction")
   system that can spot the rules that have been useful, and further
   refine them, or spot weak rules, and reject them.
   (See also nlp/seme/README)

Steps 1) and 2) are largely (but incompletely, imperfectly) addressed
by code described in the nlp/triples/README. Step 3) is largely sovled
by code in the "persist" directory.  Step 4) is being addressed in
nlp/seme/README.

At this point, this directory should probably be just a place-holder
for code that implements the Hobbs anaphora resolution algo within 
OpenCog.  This code is yet to be written. Note that the Hobbs algo
is currently implemented within RelEx.

Thus, this directory is mostly just empty, for now.

XXXX
The fourth step seems to be the vaguest at this point. I hope to
combine ideas making use of mutual information, Markov networks and
Markov logic networks, and heirarchical feature selection to accomplish
this. The general idea is to be able to spot commonly-occuring patterns
or features, in much the same way that one might find patterns in
genetic data. See, [FEAT], and in particular [HCMI] (square brackets
denote references at end of text, below).

It is expected that the system will be initially primed with a number
of hand-generated rules.

The first three issues are significant, practical barrirers to further
progress, and are thus tackled first.


Statistical Information, Databases
==================================
What information needs to be collected, and how it should be correllated.

Need to store:
-- mutual information ...  but how??  
See ref [HCMI]



References
==========
Some background reading for various keywords used here:

[FEAT] Feature extraction. See
   http://en.wikipedia.org/wiki/Feature_extraction
   http://en.wikipedia.org/wiki/Cluster_analysis

[HCMI]
   Alexander Kraskov, Harald St√∂gbauer, Ralph G. Andrzejak,
   Peter Grassberger, "Hierarchical Clustering Based on Mutual
   Information" (2003) http://arxiv.org/abs/q-bio/0311039

Alexander Yates and Oren Etzioni.
[http://www.cis.temple.edu/~yates/papers/resolver-jair09.pdf
Unsupervised Methods for Determining Object and Relation Synonyms on
the Web]. Journal of Artificial Intelligence Research 34, March,
2009, pages 255-296.

